{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Generalization Analysis\n",
    "\n",
    "This notebook evaluates how models trained on week T generalize to future weeks (T+1, T+2, ...).\n",
    "\n",
    "We analyze models from `exps/cesnet_v3/` to understand temporal drift and model degradation over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "from temporal_generalization import (\n",
    "    evaluate_temporal_generalization,\n",
    "    results_to_dataframe,\n",
    "    save_results,\n",
    "    load_results,\n",
    "    extract_week_number\n",
    ")\n",
    "from train_per_week_cesnet import load_label_mapping\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (16, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "EXPERIMENT_DIR = Path('exps/cesnet_v3')\n",
    "DATASET_ROOT = Path('/home/anatbr/dataset/CESNET-TLS-Year22')\n",
    "RESULTS_PATH = Path('exps/cesnet_v3_temporal_generalization_results.json')\n",
    "\n",
    "# Evaluation parameters\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 8\n",
    "RESOLUTION = 256\n",
    "DATA_SAMPLE_FRAC = 0.1  # Use 10% of each week's data for faster evaluation\n",
    "SEED = 42\n",
    "DEVICE = 'cuda:0'\n",
    "CHECKPOINT_NAME = 'best_model.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Label Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 180\n"
     ]
    }
   ],
   "source": [
    "label_indices_mapping, num_classes = load_label_mapping(DATASET_ROOT)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation (or Load Cached Results)\n",
    "\n",
    "This step evaluates all models on all weeks. It may take a while, so results are cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running temporal generalization evaluation...\n",
      "This may take a while. Results will be cached for future use.\n",
      "\n",
      "\n",
      "Found 53 weeks with data: WEEK-2022-00 to WEEK-2022-52\n",
      "Found 53 trained models\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating models:   0%|          | 0/53 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "# Check if results already exist\n",
    "if RESULTS_PATH.exists():\n",
    "    print(f\"Loading cached results from {RESULTS_PATH}\")\n",
    "    results = load_results(RESULTS_PATH)\n",
    "    print(f\"Loaded {len(results)} evaluation results\")\n",
    "else:\n",
    "    print(\"Running temporal generalization evaluation...\")\n",
    "    print(\"This may take a while. Results will be cached for future use.\\n\")\n",
    "    \n",
    "    results = evaluate_temporal_generalization(\n",
    "        experiment_dir=EXPERIMENT_DIR,\n",
    "        dataset_root=DATASET_ROOT,\n",
    "        label_indices_mapping=label_indices_mapping,\n",
    "        num_classes=num_classes,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        resolution=RESOLUTION,\n",
    "        data_sample_frac=DATA_SAMPLE_FRAC,\n",
    "        seed=SEED,\n",
    "        device=DEVICE,\n",
    "        checkpoint_name=CHECKPOINT_NAME\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    save_results(results, RESULTS_PATH)\n",
    "    print(f\"\\nCompleted {len(results)} evaluations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to DataFrame for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results_to_dataframe(results)\n",
    "print(f\"Total evaluations: {len(df)}\")\n",
    "print(f\"\\nTrain weeks: {sorted(df['train_week_num'].unique())}\")\n",
    "print(f\"Test weeks: {sorted(df['test_week_num'].unique())}\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Generalization Plot\n",
    "\n",
    "For each model trained on week T, plot its performance on all future weeks (T+1, T+2, ...).\n",
    "- Validation accuracy on training week T is marked with a star (★)\n",
    "- Test accuracy on week T+1 is highlighted with a thicker black line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_temporal_generalization(df, figsize=(18, 10), y_range=(0.7, 1.0)):\n",
    "    \"\"\"\n",
    "    Plot temporal generalization: accuracy vs week number.\n",
    "    \n",
    "    For each model trained on week T, show performance on all future weeks.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Get unique training weeks\n",
    "    train_weeks = sorted(df['train_week_num'].unique())\n",
    "    \n",
    "    # Color map for different training weeks\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(train_weeks)))\n",
    "    \n",
    "    for i, train_week in enumerate(train_weeks):\n",
    "        # Filter data for this training week\n",
    "        train_df = df[df['train_week_num'] == train_week].sort_values('test_week_num')\n",
    "        \n",
    "        if len(train_df) == 0:\n",
    "            continue\n",
    "        \n",
    "        test_weeks = train_df['test_week_num'].values\n",
    "        accuracies = train_df['accuracy'].values / 100.0  # Convert to 0-1 scale\n",
    "        \n",
    "        # Plot the line\n",
    "        label = f'Trained on W{train_week}'\n",
    "        ax.plot(test_weeks, accuracies, '-o', color=colors[i], \n",
    "                label=label, alpha=0.7, linewidth=2, markersize=4)\n",
    "        \n",
    "        # Mark validation accuracy on training week with a star\n",
    "        train_week_idx = np.where(test_weeks == train_week)[0]\n",
    "        if len(train_week_idx) > 0:\n",
    "            idx = train_week_idx[0]\n",
    "            ax.scatter(test_weeks[idx], accuracies[idx], \n",
    "                      marker='*', s=300, color=colors[i], \n",
    "                      edgecolors='black', linewidths=1.5, zorder=10)\n",
    "        \n",
    "        # Highlight test accuracy on week T+1 with black outline\n",
    "        next_week_idx = np.where(test_weeks == train_week + 1)[0]\n",
    "        if len(next_week_idx) > 0:\n",
    "            idx = next_week_idx[0]\n",
    "            ax.scatter(test_weeks[idx], accuracies[idx], \n",
    "                      marker='o', s=150, facecolors=colors[i], \n",
    "                      edgecolors='black', linewidths=2.5, zorder=9)\n",
    "    \n",
    "    ax.set_xlabel('Test Week Number', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Classification Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax.set_title('Temporal Generalization: Model Performance on Future Weeks\\n' + \n",
    "                 '★ = Validation accuracy on training week | ● with thick border = Test accuracy on week T+1',\n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    ax.set_ylim(y_range)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Legend\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', \n",
    "             fontsize=10, framealpha=0.9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "fig = plot_temporal_generalization(df)\n",
    "plt.savefig('exps/temporal_generalization_plot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Plot saved to exps/temporal_generalization_plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Degradation Analysis\n",
    "\n",
    "Analyze how much performance degrades over time (weeks after training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weeks elapsed since training\n",
    "df['weeks_elapsed'] = df['test_week_num'] - df['train_week_num']\n",
    "\n",
    "# Filter only future weeks (excluding training week itself)\n",
    "future_df = df[df['weeks_elapsed'] > 0].copy()\n",
    "\n",
    "# Group by weeks elapsed and calculate statistics\n",
    "degradation = future_df.groupby('weeks_elapsed')['accuracy'].agg(['mean', 'std', 'count'])\n",
    "degradation = degradation.reset_index()\n",
    "\n",
    "print(\"Performance degradation over time:\")\n",
    "print(degradation.head(10))\n",
    "\n",
    "# Plot degradation\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.errorbar(degradation['weeks_elapsed'], degradation['mean'], \n",
    "            yerr=degradation['std'], marker='o', capsize=5, linewidth=2)\n",
    "ax.set_xlabel('Weeks After Training', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Mean Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Average Performance Degradation Over Time', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('exps/performance_degradation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap: Train Week vs Test Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pivot table for heatmap\n",
    "heatmap_data = df.pivot(index='train_week_num', columns='test_week_num', values='accuracy')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 12))\n",
    "sns.heatmap(heatmap_data, annot=False, fmt='.1f', cmap='RdYlGn', \n",
    "            vmin=70, vmax=100, cbar_kws={'label': 'Accuracy (%)'},\n",
    "            ax=ax)\n",
    "ax.set_xlabel('Test Week Number', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Train Week Number', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Model Accuracy Heatmap: Training Week vs Test Week', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('exps/accuracy_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance on same week (validation)\n",
    "same_week = df[df['train_week_num'] == df['test_week_num']]\n",
    "print(\"Performance on same week (validation):\")\n",
    "print(f\"  Mean: {same_week['accuracy'].mean():.2f}%\")\n",
    "print(f\"  Std:  {same_week['accuracy'].std():.2f}%\")\n",
    "print(f\"  Min:  {same_week['accuracy'].min():.2f}%\")\n",
    "print(f\"  Max:  {same_week['accuracy'].max():.2f}%\")\n",
    "\n",
    "# Performance on next week (T+1)\n",
    "next_week = df[df['weeks_elapsed'] == 1]\n",
    "print(\"\\nPerformance on next week (T+1):\")\n",
    "print(f\"  Mean: {next_week['accuracy'].mean():.2f}%\")\n",
    "print(f\"  Std:  {next_week['accuracy'].std():.2f}%\")\n",
    "print(f\"  Min:  {next_week['accuracy'].min():.2f}%\")\n",
    "print(f\"  Max:  {next_week['accuracy'].max():.2f}%\")\n",
    "\n",
    "# Average drop from validation to T+1\n",
    "merged = same_week.merge(next_week, on='train_week_num', suffixes=('_val', '_next'))\n",
    "merged['drop'] = merged['accuracy_val'] - merged['accuracy_next']\n",
    "print(\"\\nAccuracy drop from validation to T+1:\")\n",
    "print(f\"  Mean: {merged['drop'].mean():.2f} percentage points\")\n",
    "print(f\"  Std:  {merged['drop'].std():.2f} percentage points\")\n",
    "\n",
    "# Long-term performance (4+ weeks)\n",
    "long_term = df[df['weeks_elapsed'] >= 4]\n",
    "if len(long_term) > 0:\n",
    "    print(\"\\nPerformance 4+ weeks after training:\")\n",
    "    print(f\"  Mean: {long_term['accuracy'].mean():.2f}%\")\n",
    "    print(f\"  Std:  {long_term['accuracy'].std():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best and Worst Generalizing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average performance on future weeks for each training week\n",
    "future_performance = future_df.groupby('train_week_num')['accuracy'].agg(['mean', 'std', 'count'])\n",
    "future_performance = future_performance.reset_index()\n",
    "future_performance = future_performance.sort_values('mean', ascending=False)\n",
    "\n",
    "print(\"Models with best average performance on future weeks:\")\n",
    "print(future_performance.head(5))\n",
    "\n",
    "print(\"\\nModels with worst average performance on future weeks:\")\n",
    "print(future_performance.tail(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
